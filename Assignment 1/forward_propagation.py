"""
Performs forward propagation
"""
import numpy as np
from numpy import random

"""-PARAMS-"""


def initialize_parameters(layer_dims):
    """
    Hint: Use the randn and zeros functions of numpy to initialize W and b, respectively
    input: an array of the dimensions of each layer in the network (layer 0 is the size of the flattened input, layer L is the output softmax)
    output: a dictionary containing the initialized W and b parameters of each layer (W1‚Ä¶WL, b1‚Ä¶bL).
    """
    random.seed(1994)
    params_W = {f"W{i + 1}": random.randn(layer_dims[i + 1], layer_dims[i]) * np.sqrt(2 / layer_dims[i]) for i in
                range(len(layer_dims) - 1)}
    params_b = {f"b{i + 1}": np.zeros((layer_dims[i + 1], 1)) for i in range(len(layer_dims) - 1)}
    params_output = {}
    params_output.update(params_W)
    params_output.update(params_b)
    return params_output


def linear_forward(A, W, b):
    """
    Implement the linear part of a layer's forward propagation.
    :param A: the activations of the previous layer
    :param W: the weight matrix of the current layer (of shape [size of current layer, size of previous layer])
    :param b: the bias vector of the current layer (of shape [size of current layer, 1])
    :return:
        Z ‚Äì the linear component of the activation function (i.e., the value before applying the non-linear function)
        linear_cache ‚Äì a dictionary containing A, W, b (stored for making the backpropagation easier to compute)
    """
    # ùëß = ùë§^ùëá*ùë• + ùëè
    Z = np.matmul(W, A) + b  # W is already transposed
    linear_cache = {"A": A, "W": W, "b": b}
    return Z, linear_cache


def softmax(Z):
    """
    note:
    Softmax can be thought of as a sigmoid for multi-class problems. The formula for softmax for each node in the output layer is as follows:
    Softmax„Äñ(z)„Äó_i=(exp‚Å°(z_i))/(‚àë_j‚ñí„Äñexp‚Å°(z_j)„Äó)
    :param Z: the linear component of the activation function
    :return:
        A ‚Äì the activations of the layer
        activation_cache ‚Äì returns Z, which will be useful for the backpropagation
    """
    e_Z = np.exp(Z - Z.max(axis=0))
    c = np.sum(e_Z, axis=0)
    A = e_Z / c
    activation_cache = {"Z": Z}
    return A, activation_cache


def relu(Z):
    """
    :param Z: the linear component of the activation function
    :return:
        A ‚Äì the activations of the layer
        activation_cache ‚Äì returns Z, which will be useful for the backpropagation
    """
    A = np.maximum(Z, 0)
    activation_cache = {"Z": Z}
    return A, activation_cache


def linear_activation_forward(A_prev, W, B, activation):
    """
    Implement the forward propagation for the LINEAR->ACTIVATION layer
    :param A_prev: activations of the previous layer
    :param W: the weights matrix of the current layer
    :param B: the bias vector of the current layer
    :param activation: the activation function to be used (a string, either ‚Äúsoftmax‚Äù or ‚Äúrelu‚Äù)
    :return:
        A ‚Äì the activations of the current layer
        cache ‚Äì a joint dictionary containing both linear_cache and activation_cache
    """
    Z, linear_cache = linear_forward(A_prev, W, B)
    A, activation_cache = softmax(Z) if activation == "softmax" else relu(Z)
    cache = {}
    cache.update(linear_cache)
    cache.update(activation_cache)

    return A, cache


def L_model_forward(X, parameters, use_batchnorm, use_dropout=False):
    """
    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation
    :param X: the data, numpy array of shape (input size, number of examples)
    :param parameters: the initialized W and b parameters of each layer
    :param use_batchnorm: a boolean flag used to determine whether to apply batchnorm after the activation
        (note that this option needs to be set to ‚Äúfalse‚Äù in Section 3 and ‚Äútrue‚Äù in Section 4).
    :return:
        AL ‚Äì the last post-activation value
        caches ‚Äì a list of all the cache objects generated by the linear_forward function
    """
    caches = list()
    layers_n = len(parameters) // 2

    # all layers but the last should have the ReLU activation function,
    # and the final layer will apply the softmax activation function.
    activation_functions = ["relu" for i in range(layers_n - 1)]
    activation_functions.append("softmax")

    AL = X
    for layer_i, activation in zip(range(1, layers_n + 1), activation_functions):
        AL, cache = linear_activation_forward(AL,
                                              parameters.get(f"W{layer_i}"),
                                              parameters.get(f"b{layer_i}"),
                                              activation)
        caches.append(cache)
        if use_batchnorm:
            AL = apply_batchnorm(AL) if activation == "relu" else AL
        if use_dropout:
            AL = apply_dropout(AL) if activation == "relu" else AL

    return AL, caches


def compute_cost(AL, Y):
    """
    	Implement the cost function defined by equation. The requested cost function is categorical cross-entropy loss.
    	The formula is as follows :
        cost=-1/m*‚àë_1^m‚ñí‚àë_1^C‚ñí„Äñy_i  log‚Å°„Äñ(y ÃÇ)„Äó,
        where y_i is one for the true class (‚Äúground-truth‚Äù) and y ÃÇ is the softmax-adjusted prediction
        (this link provides a good overview).
    :param AL: probability vector corresponding to your label predictions, shape (num_of_classes, number of examples)
    :param Y: the labels vector (i.e. the ground truth)
    :return:
        cost ‚Äì the cross-entropy cost
    """
    m = AL.shape[0]  # num_of_classes
    cost = (-1 * np.sum(Y * np.log(AL))) / m

    return cost


def apply_batchnorm(A):
    """
    performs batchnorm on the received activation values of a given layer.
    :param A: the activation values of a given layer
    :return:
        NA - the normalized activation values, based on the formula learned in class
    """
    mean = np.mean(A, axis=1)
    variance = np.var(A, axis=1)
    epsilon = 0.000000001
    NA = (A - np.expand_dims(mean, axis=1)) / np.sqrt(np.expand_dims(variance, axis=1) + epsilon)
    return NA


def apply_dropout(AL):
    """
    Performs dropout, as learned in the lectures
    :param A: the activation values of a given layer
    :return:
        a_i - the dropout activation values, based on the formula learned in class
    """
    keep_prob = 0.98
    d_i = random.rand(AL.shape[0], AL.shape[1]) < keep_prob
    a_i = np.multiply(AL, d_i)
    a_i = a_i / keep_prob
    return a_i
